<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Real-time Audio Analyzer</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        #transcript, .response-area {
            width: 100%;
            height: 200px;
            margin-top: 0px;
            font-size: 16px;
        }
        button {
            padding: 10px 20px;
            font-size: 16px;
        }
        .status-indicator {
            display: inline-block;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            margin-right: 8px;
        }
        .status-indicator.active {
            background-color: #FF0000;
            animation: pulse 1.5s infinite;
        }
        .status-indicator.inactive {
            background-color: #CCCCCC;
        }
        #permissionInfo {
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 5px;
            margin: 10px 0;
            display: none;
        }
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.5; }
            100% { opacity: 1; }
        }
    </style>
</head>
<body>
    <h1>Real-time Audio Analyzer</h1>
    
    <div id="permissionInfo">
        <p><strong>Microphone access is required.</strong> Please allow microphone access when prompted.</p>
        <p>If you denied permission previously and want to enable it:</p>
        <ol>
            <li>Click the lock/info icon in your browser's address bar</li>
            <li>Find the microphone settings and change to "Allow"</li>
            <li>Refresh this page</li>
        </ol>
    </div>
    
    <div>
        <button id="startButton">Start Transcription</button>
        <button id="stopButton" disabled>Stop Transcription</button>
        <button onclick="copyTranscript()">Copy Transcript</button>
        <button onclick="document.getElementById('transcript').value='';">Clear Transcript</button>
        <span id="status">
            <span class="status-indicator inactive" id="statusIndicator"></span>
            <span id="statusText">Not recording</span>
        </span>
    </div>

    <BR>
    
    <input type="checkbox" id="showInterim" checked hidden>

    <div class="control-group">
        <label for="confidenceThreshold">Confidence Threshold:</label>
        <input type="range" id="confidenceThreshold" min="0.00" max="1" step="0.05" value="0.00">
        <span id="confidenceValue">0.00</span>
        <span class="tooltip">Higher values mean only more confident transcriptions are shown</span>
    </div>
    <BR>
    <label>AI Model</label>
    <select id="modelSelect" class="select" width="100%">
    </select>
     
    <label>AI Analysis Time Delay</label>
    <select id="analysis_time_delay" class="select" width="100%">
        <option value="10000">10 Seconds</option>
        <option value="30000" selected>30 Seconds</option>
        <option value="60000">1 Minute</option>
        <option value="3600000">1 Hour</option>
    </select>
     
    <label for="language">Language</label>
    <select id="language" name="language">
        <option value="af">Afrikaans</option>
        <option value="sq">Albanian</option>
        <option value="am">Amharic</option>
        <option value="ar">Arabic</option>
        <option value="hy">Armenian</option>
        <option value="eu">Basque</option>
        <option value="bn">Bengali</option>
        <option value="bs">Bosnian</option>
        <option value="bg">Bulgarian</option>
        <option value="ca">Catalan</option>
        <option value="zh-CN">Chinese (Simplified)</option>
        <option value="zh-TW">Chinese (Traditional)</option>
        <option value="hr">Croatian</option>
        <option value="cs">Czech</option>
        <option value="da">Danish</option>
        <option value="nl">Dutch</option>
        <option value="en" selected>English</option>
        <option value="et">Estonian</option>
        <option value="fi">Finnish</option>
        <option value="fr">French</option>
        <option value="ka">Georgian</option>
        <option value="de">German</option>
        <option value="el">Greek</option>
        <option value="gu">Gujarati</option>
        <option value="ht">Haitian Creole</option>
        <option value="he">Hebrew</option>
        <option value="hi">Hindi</option>
        <option value="hu">Hungarian</option>
        <option value="is">Icelandic</option>
        <option value="id">Indonesian</option>
        <option value="ga">Irish</option>
        <option value="it">Italian</option>
        <option value="ja">Japanese</option>
        <option value="jw">Javanese</option>
        <option value="kn">Kannada</option>
        <option value="kk">Kazakh</option>
        <option value="km">Khmer</option>
        <option value="ko">Korean</option>
        <option value="ku">Kurdish (Kurmanji)</option>
        <option value="lv">Latvian</option>
        <option value="lt">Lithuanian</option>
        <option value="mk">Macedonian</option>
        <option value="ms">Malay</option>
        <option value="ml">Malayalam</option>
        <option value="mr">Marathi</option>
        <option value="mn">Mongolian</option>
        <option value="ne">Nepali</option>
        <option value="no">Norwegian</option>
        <option value="ps">Pashto</option>
        <option value="fa">Persian</option>
        <option value="pl">Polish</option>
        <option value="pt">Portuguese</option>
        <option value="pa">Punjabi</option>
        <option value="ro">Romanian</option>
        <option value="ru">Russian</option>
        <option value="sr">Serbian</option>
        <option value="si">Sinhala</option>
        <option value="sk">Slovak</option>
        <option value="sl">Slovenian</option>
        <option value="es">Spanish</option>
        <option value="sw">Swahili</option>
        <option value="sv">Swedish</option>
        <option value="ta">Tamil</option>
        <option value="te">Telugu</option>
        <option value="th">Thai</option>
        <option value="tr">Turkish</option>
        <option value="uk">Ukrainian</option>
        <option value="ur">Urdu</option>
        <option value="uz">Uzbek</option>
        <option value="vi">Vietnamese</option>
        <option value="cy">Welsh</option>
        <option value="xh">Xhosa</option>
        <option value="yi">Yiddish</option>
        <option value="yo">Yoruba</option>
        <option value="zu">Zulu</option>
    </select>

    <BR><BR>
    Raw Transcript
    <textarea id="transcript" readonly></textarea>
    
    <div class="interim-container">
        <h4>Interim Results</h4>
        <div id="interimDisplay" class="interim-text"></div>
    </div>

    <h2>AI Analysis</h2>
    <button onclick="sendToAI(document.getElementById('transcript').value);">Analyze Now</button>

    <BR>
    <BR>
    <label>AI API Response</label>
    <textarea id="aiAPIResponse" class="response-area" readonly></textarea>
    <BR>
    <label>Analyzed Transcript</label>
    <textarea id="fullTranscript" class="response-area" readonly></textarea>
    <BR>
    <label>Current Idea</label>
    <textarea id="current_idea" class="response-area" readonly></textarea>
    <BR>
    <label>Summary</label>
    <textarea id="summary" class="response-area" readonly></textarea>
    <BR>
    <label>Secrets</label>
    <textarea id="secrets" class="response-area" readonly></textarea>
    <BR>
    <label>Contradictions</label>
    <textarea id="contradictions" class="response-area" readonly></textarea>
    <BR>
    <label>Intent</label>
    <textarea id="intent" class="response-area" readonly></textarea>
    <BR>
    <label>Expected Response</label>
    <textarea id="expectedResponse" class="response-area" readonly></textarea>
    <BR>
    <label>Reward</label>
    <textarea id="reward" class="response-area" readonly></textarea>
    
    <script>
        // Global variables for speech recognition
        let recognition;
        let isRecognizing = false;
        let lastRequestTime = 0;
        let recognitionPaused = false;
        let lastTranscript = '';
        let restartAttempts = 0;
        let maxRestartAttempts = 10; // Increased max attempts
        let restartDelay = 1000; // Base delay in ms
        let watchdogTimer = null;
        let lastChatTime = Date.now();
        
        // Create a permission state variable
        let micPermissionState = 'unknown'; // 'unknown', 'granted', 'denied', 'prompt'
        
        // Check if the browser supports speech recognition
        const supportsSpeechRecognition = 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window;
        
        if (!supportsSpeechRecognition) {
            alert('Your browser does not support the Web Speech API. Please use Chrome, Edge, or Safari.');
            document.getElementById('startButton').disabled = true;
        }

        // Watchdog function to monitor if speech recognition is still active
        async function startWatchdog() {
            if (watchdogTimer) {
                clearInterval(watchdogTimer);
            }

            watchdogTimer = setInterval(async () => {
                if (isRecognizing && !recognitionPaused) {
                    const timeSinceLastActivity = Date.now() - lastChatTime;
                    if (timeSinceLastActivity > 7000) { // 7 seconds of inactivity
                        lastChatTime = Date.now();
                        console.log('Watchdog detected inactivity, restarting recognition');

                        stopTranscription();
                        await sleep(2000);
                        startTranscription();
                    }
                }
            }, 1000);
        }


        function sleep(ms) {
            new Promise(resolve => setTimeout(resolve, ms));
        }

        function stopWatchdog() {
            if (watchdogTimer) {
                clearInterval(watchdogTimer);
                watchdogTimer = null;
            }
        }

        function copyTranscript() {
            var copyText = document.getElementById("transcript");
            copyText.select();
            copyText.setSelectionRange(0, 99999);
            navigator.clipboard.writeText(copyText.value);
        }

        function detectRepeatedPhrases(transcript) {
            const phrases = transcript.split('\n').filter(Boolean); // Split by newlines and filter out empty strings
            const phraseCounts = {};
            let repeatedPhrases = [];

            phrases.forEach(phrase => {
                if (phraseCounts[phrase]) {
                    phraseCounts[phrase]++;
                } else {
                    phraseCounts[phrase] = 1;
                }
            });

            // Collect phrases that appear more than once
            for (const phrase in phraseCounts) {
                if (phraseCounts[phrase] > 1) {
                    repeatedPhrases.push(phrase);
                }
            }
            return repeatedPhrases;
        }

        // Check microphone permission when the page loads
        async function checkMicrophonePermission() {
            try {
                const permissionStatus = await navigator.permissions.query({ name: 'microphone' });
                
                micPermissionState = permissionStatus.state;
                
                // Set up listener for permission changes
                permissionStatus.onchange = function() {
                    micPermissionState = this.state;
                    updatePermissionUI();
                    
                    // If permission was just granted and we were trying to record, start recording
                    if (this.state === 'granted' && recognitionPaused) {
                        recognitionPaused = false;
                        startTranscription();
                    }
                };
                
                updatePermissionUI();
                return micPermissionState;
            } catch (error) {
                console.error('Error checking microphone permission:', error);
                // Fallback - we'll need to try to access the microphone to know the permission
                return 'unknown';
            }
        }

        function updatePermissionUI() {
            const permissionInfo = document.getElementById('permissionInfo');
            
            switch(micPermissionState) {
                case 'granted':
                    permissionInfo.style.display = 'none';
                    document.getElementById('startButton').disabled = false;
                    break;
                case 'denied':
                    permissionInfo.style.display = 'block';
                    document.getElementById('startButton').disabled = true;
                    break;
                case 'prompt':
                    permissionInfo.style.display = 'block';
                    document.getElementById('startButton').disabled = false;
                    break;
                default:
                    permissionInfo.style.display = 'none';
                    document.getElementById('startButton').disabled = false;
            }
        }

        function startTranscription() {
            if (!supportsSpeechRecognition) {
                alert('Your browser does not support the Web Speech API.');
                return;
            }

            // Try to get microphone permission first
            navigator.mediaDevices.getUserMedia({ audio: true })
                .then(stream => {
                    // Permission granted, clean up the stream and start recognition
                    stream.getTracks().forEach(track => track.stop());
                    micPermissionState = 'granted';
                    updatePermissionUI();
                    initializeRecognition();
                })
                .catch(error => {
                    console.error('Microphone permission error:', error);
                    
                    if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
                        micPermissionState = 'denied';
                    } else {
                        micPermissionState = 'prompt';
                    }
                    
                    updatePermissionUI();
                    recognitionPaused = true;
                    
                    if (micPermissionState === 'prompt') {
                        // We can still try to start recognition, which will trigger the permission prompt
                        initializeRecognition();
                    }
                });
        }

        function initializeRecognition() {
            // Stop any existing recognition instance
            if (recognition) {
                recognition.stop();
            }

            // Reset state
            isRecognizing = false;
            
            // Create the recognition object
            recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = document.getElementById('language').value;

            // Get the interim results display preference
            const showInterim = document.getElementById('showInterim').checked;
            // Get confidence threshold if available
            const confidenceThreshold = parseFloat(document.getElementById('confidenceThreshold').value || 0);

            recognition.onresult = function(event) {

                lastChatTime = Date.now();

                console.log('Recognition result received', event);
                
                // Update the last activity time
                //lastRequestTime = Date.now();
                
                let newTranscript = ''; // New transcript that we'll append to the textarea
                let interimTranscript = ''; // For storing interim results separately
                
                // Store the current value of the transcript
                lastTranscript = document.getElementById('transcript').value.trim();

                // Loop through all the results from the event object
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const result = event.results[i];
                    const confidence = result[0].confidence;
                    
                    // Skip results with confidence below threshold
                    if (confidence < confidenceThreshold) {
                        continue;
                    }
                    
                    if (result.isFinal) {
                        // This is a final result
                        let recognizedPhrase = result[0].transcript.trim();

                        // Skip if the recognized phrase is empty
                        if (!recognizedPhrase) {
                            continue;
                        }

                        // Check if this phrase has already been added
                        const lastFewLines = lastTranscript.split('\n').slice(-3).join('\n');
                        if (!lastFewLines.includes(recognizedPhrase)) {
                            // Append the new phrase to the transcript string
                            newTranscript += recognizedPhrase + '\n';

                            // Optional: Log the recognized phrase for debugging
                            console.log('Recognized (Final):', recognizedPhrase, 'Confidence:', confidence);
                        }
                    } else if (showInterim) {
                        // This is an interim result, only add if user opted to show interim results
                        let recognizedPhrase = result[0].transcript.trim();
                        if (recognizedPhrase) {
                            interimTranscript += recognizedPhrase + ' ';
                            console.log('Interim:', recognizedPhrase, 'Confidence:', confidence);
                        }
                    }
                }

                // Update the textarea with the new transcript if there is one
                if (newTranscript) {
                    document.getElementById('transcript').value += newTranscript;
                    
                    // Reset restart attempts counter since we're receiving results
                    restartAttempts = 0;
                    restartDelay = 1000; // Reset the base delay
                }
                
                // Display interim results in a separate element if enabled
                if (showInterim && document.getElementById('interimDisplay')) {
                    document.getElementById('interimDisplay').textContent = interimTranscript;
                }

                // Auto-scroll to the bottom
                document.getElementById('transcript').scrollTop = document.getElementById('transcript').scrollHeight;

                // Check if enough time has passed to send the transcript for processing (AI, etc.)
                if (Date.now() - lastRequestTime > document.getElementById('analysis_time_delay').value) {
                    lastRequestTime = Date.now();
                    sendToAI(document.getElementById('transcript').value); // Send to AI function
                }
                else{
                    console.log(Date.now() - lastRequestTime);
                }
            };

            // Handle errors more gracefully with backoff retry
            recognition.onerror = function(event) {
                console.error('Speech recognition error:', event.error);
                
                // Different handling based on error type
                if (event.error === 'no-speech') {
                    // No speech detected - just restart
                    restartRecognition();
                } else if (event.error === 'audio-capture') {
                    updateStatus('Microphone error', false);
                    // Try to restart recognition after a delay
                    setTimeout(() => {
                        restartRecognition();
                    }, restartDelay);
                } else if (event.error === 'network') {
                    updateStatus('Network error, retrying...', false);
                    // Network error - try to restart with backoff
                    setTimeout(() => {
                        restartRecognition();
                    }, restartDelay);
                    
                    // Increase backoff delay for next attempt
                    restartDelay = Math.min(restartDelay * 2, 30000); // Max 30 seconds
                } else if (event.error === 'not-allowed') {
                    updateStatus('Microphone access denied', false);
                    micPermissionState = 'denied';
                    updatePermissionUI();
                    recognitionPaused = true;
                } else {
                    // Other errors - attempt to restart
                    restartRecognition();
                }
            };

            // Automatically restart when recognition ends
            recognition.onend = function() {
                console.log('Speech recognition ended');
                
                // Only try to restart if we're still supposed to be recognizing
                if (isRecognizing && !recognitionPaused) {
                    restartRecognition();
                } else {
                    updateStatus('Not recording', false);
                }
            };

            // Start the recognition
            try {
                recognition.start();
                console.log('Speech recognition started');
                isRecognizing = true;
                recognitionPaused = false;
                
                // Update UI
                document.getElementById('startButton').disabled = true;
                document.getElementById('stopButton').disabled = false;
                updateStatus('Recording...', true);
                
                // Reset counters
                restartAttempts = 0;
                restartDelay = 1000;
                
                // Start the watchdog timer
                startWatchdog();
                
                // Initialize the last request time
                lastRequestTime = Date.now();
            } catch (e) {
                console.error('Error starting recognition:', e);
                alert('Failed to start speech recognition. Please reload the page and try again.');
                updateStatus('Error starting', false);
            }
        }

        async function restartRecognition(force = false) {

            stopTranscription();
            await sleep(2000);
            startTranscription();

            /*
            if (!isRecognizing || recognitionPaused) {
                return;
            }
            
            // If we've exceeded the max attempts and it's not a forced restart
            if (restartAttempts >= maxRestartAttempts && !force) {
                console.log('Max restart attempts reached, stopping recognition');
                updateStatus('Too many restarts, stopped', false);
                stopTranscription();
                return;
            }
            
            restartAttempts++;
            console.log(`Restarting recognition, attempt ${restartAttempts}`);
            
            try {
                // Make sure the old recognition instance is stopped
                if (recognition) {
                    recognition.stop();
                }
                
                // Small delay before restarting
                setTimeout(() => {
                    // Create a new recognition instance and start it
                    initializeRecognition();
                }, 500);
            } catch (e) {
                console.error('Error restarting recognition:', e);
                
                // If we can't restart, try again with a longer delay
                setTimeout(() => {
                    restartRecognition();
                }, restartDelay);
                
                // Increase the delay for the next attempt
                restartDelay = Math.min(restartDelay * 2, 30000); // Max 30 seconds
            }
            */
        }

        function stopTranscription() {
            if (recognition) {
                recognition.stop();
            }
            
            isRecognizing = false;
            recognitionPaused = false;
            
            // Update UI
            document.getElementById('startButton').disabled = false;
            document.getElementById('stopButton').disabled = true;
            updateStatus('Not recording', false);
            
            // Stop the watchdog
            stopWatchdog();
        }

        async function updateStatus(message, isActive) {
            const statusText = document.getElementById('statusText');
            const statusIndicator = document.getElementById('statusIndicator');
            
            statusText.textContent = message;
            
            if (isActive) {
                statusIndicator.classList.remove('inactive');
                statusIndicator.classList.add('active');
            } else {
                statusIndicator.classList.remove('active');
                statusIndicator.classList.add('inactive');
            }
        }

        async function sendToAI(transcript) {

            function estimateTokens(message) {
                return Math.ceil(message.split(/\s+/).length / 1.33);
            }

            while (estimateTokens(transcript) > 1000) {
                transcript = transcript.substring(1);
            }

            console.log("Trimmed Transcript: " + transcript);

            if (!transcript.trim()) {
                return; // Don't send empty transcripts
            }
            
            updateStatus('Sending to AI...', true);
            
            fetch("https://proxy.small-recipe-9582.workers.dev/", {
                method: "POST",
                headers: {
                    "accept-language": "en-US,en;q=0.9",
                    "authorization": "Bearer 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",
                    "content-type": "application/json"
                },
                referrer: "https://www.avemariacloud.com/",
                referrerPolicy: "strict-origin-when-cross-origin",
                body: JSON.stringify({
                    model: document.getElementById("modelSelect").value,
                    messages: [
                        {
                            role: "system",
                            content: `I am interested in analyzing some transcription data

I am interested in creating a full word for word transcript, not a summary or analysis, of what has been said based on the transcription data.

I am interested in a single sentence summary of the most important idea currently being discussed (Current_Idea).

I am interested in a three sentence summary of the entire transcript (Full_Summary).

I am interested in detecting secrets and/or contradictions. I am interested in a single sentence response.

I am interested in detecting the intent of the speaker. I am interested in a single sentence response.

I am interested in detecting the ideal response that this person is expecting. I am interested in a response that is consistent with a conversation. I am interested in a single sentence response.

I am interested in some sort of intellectual reward from this speech. I am interested in a single sentence response.

Respond in JSON with the labels: Full_Transcript, Current_Idea, Full_Summary, Secrets, Contradictions, Intent, Expected_Response, Reward.`
                        },
                        {
                            role: "user",
                            content: transcript
                        }
                    ],
                    response_format: { type: "json_object" }
                }),
                mode: "cors"
            })
            .then(response => response.json())
            .then(data => {
                displayResponse(data);
                updateStatus(isRecognizing ? 'Recording...' : 'Not recording', isRecognizing);
            })
            .catch(error => {
                console.error('Error:', error);
                updateStatus(isRecognizing ? 'Recording...' : 'Not recording', isRecognizing);
            });
        }

        function displayResponse(data) {
            const now = new Date();
            const hours = now.getHours().toString().padStart(2, '0');
            const minutes = now.getMinutes().toString().padStart(2, '0');
            const seconds = now.getSeconds().toString().padStart(2, '0');

            const timeString = `${hours}:${minutes}:${seconds}`;
            console.log(timeString);

            console.log(data);

            data = data['choices'][0]['message']['content'];
            console.log(data);

            document.getElementById("aiAPIResponse").value = data;

            data = JSON.parse(data);

            document.getElementById("fullTranscript").value = data.Full_Transcript;

            document.getElementById("current_idea").value = "[" + timeString + "] " + data.Current_Idea + `

` + document.getElementById("current_idea").value;

            document.getElementById("summary").value = "[" + timeString + "] " + data.Full_Summary + `

` + document.getElementById("summary").value;

            document.getElementById("secrets").value = "[" + timeString + "] " + data.Secrets + `

` + document.getElementById("secrets").value;

            document.getElementById("contradictions").value = "[" + timeString + "] " + data.Contradictions + `

` + document.getElementById("contradictions").value;

            document.getElementById("intent").value = "[" + timeString + "] " + data.Intent + `

` + document.getElementById("intent").value;

            document.getElementById("expectedResponse").value = "[" + timeString + "] " + data.Expected_Response + `

` + document.getElementById("expectedResponse").value;

            document.getElementById("reward").value = "[" + timeString + "] " + data.Reward + `

` + document.getElementById("reward").value;
        }

        // Set up event listeners when the page loads
        window.addEventListener('DOMContentLoaded', function() {
            // Check microphone permission
            checkMicrophonePermission();
            
            // Set up event listeners for buttons
            document.getElementById('startButton').addEventListener('click', startTranscription);
            document.getElementById('stopButton').addEventListener('click', stopTranscription);
            
            // Set confidence value display
            document.getElementById('confidenceValue').textContent = document.getElementById('confidenceThreshold').value;
            
            // Update confidence value when slider changes
            document.getElementById('confidenceThreshold').addEventListener('input', function() {
                document.getElementById('confidenceValue').textContent = this.value;
            });
            
            // Load previously selected language
            const selectedLanguage = localStorage.getItem('selectedLanguage');
            if (selectedLanguage) {
                document.getElementById('language').value = selectedLanguage;
            }
            
            // Save language selection
            document.getElementById('language').addEventListener('change', function() {
                localStorage.setItem('selectedLanguage', this.value);
                
                // If currently recognizing, restart with new language
                if (isRecognizing) {
                    restartRecognition(true);
                }
            });
        });
    </script>

    <BR>
    <BR>

<div id="informationFooter" style="padding-left: 4em; padding-right: 4em;"></div>
<script src="https://www.avemariacloud.com/generic-loader.js"></script>

</body>
</html>
